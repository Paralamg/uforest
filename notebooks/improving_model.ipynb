{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec2ba803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CocoDetection\n",
    "from pycocotools.coco import COCO\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_dir = '../data/Bamberg_coco2048/coco2048'\n",
    "img_folder = f'{data_dir}/train2023'\n",
    "ann_file = f'{data_dir}/annotations/instances_tree_train2023.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f4d66b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def get_train_augs():\n",
    "    return A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.2),\n",
    "        A.Resize(512, 512),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2()\n",
    "    ], bbox_params=A.BboxParams(format='coco'))\n",
    "\n",
    "def get_val_augs():\n",
    "    return A.Compose([\n",
    "        A.Resize(512, 512),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2()\n",
    "    ], bbox_params=A.BboxParams(format='coco'))\n",
    "\n",
    "def transform(image: Image, target: dict, is_train: bool):\n",
    "    image_np = np.array(image)\n",
    "    \n",
    "    # Извлеките аннотации (маски, bboxes)\n",
    "    bboxes = [ann['bbox'] for ann in target]\n",
    "    masks = [ann['segmentation'] for ann in target]\n",
    "    \n",
    "    # Выберите аугментации в зависимости от режима\n",
    "    if is_train:\n",
    "        aug = get_train_augs()\n",
    "    else:\n",
    "        aug = get_val_augs()\n",
    "    \n",
    "    # Примените преобразования\n",
    "    transformed = aug(image=image_np, bboxes=bboxes, masks=masks)\n",
    "    return transformed['image'], transformed['bboxes'], transformed['masks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b25769f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CocoDetection\n",
    "\n",
    "class CustomCocoDataset(CocoDetection):\n",
    "    def __init__(self, root, annFile, is_train: bool = True):\n",
    "        super().__init__(root, annFile)\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, target = super().__getitem__(idx)\n",
    "        return transform(image, target, self.is_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efeec4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.40s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=1.40s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomCocoDataset(\n",
    "    root=img_folder,\n",
    "    annFile=ann_file,\n",
    "    is_train=True\n",
    ")\n",
    "\n",
    "# Валидационный датасет без аугментаций\n",
    "val_dataset = CustomCocoDataset(\n",
    "    root=img_folder,\n",
    "    annFile=ann_file,\n",
    "    is_train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c3fbbc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "All elements in masks must be numpy arrays",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Пример для тренировочного изображения\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m image, bboxes, masks = \u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTrain image shape:\u001b[39m\u001b[33m\"\u001b[39m, image.shape)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Пример для валидационного изображения\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mCustomCocoDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m      9\u001b[39m     image, target = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getitem__\u001b[39m(idx)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mtransform\u001b[39m\u001b[34m(image, target, is_train)\u001b[39m\n\u001b[32m     31\u001b[39m     aug = get_val_augs()\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Примените преобразования\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m transformed = \u001b[43maug\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbboxes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m transformed[\u001b[33m'\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m'\u001b[39m], transformed[\u001b[33m'\u001b[39m\u001b[33mbboxes\u001b[39m\u001b[33m'\u001b[39m], transformed[\u001b[33m'\u001b[39m\u001b[33mmasks\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/paralamg/845E4AB55E4AA032/Code/uforest/.venv/lib/python3.13/site-packages/albumentations/core/composition.py:607\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, force_apply, *args, **data)\u001b[39m\n\u001b[32m    604\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m need_to_run:\n\u001b[32m    605\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m    610\u001b[39m     data = t(**data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/paralamg/845E4AB55E4AA032/Code/uforest/.venv/lib/python3.13/site-packages/albumentations/core/composition.py:630\u001b[39m, in \u001b[36mCompose.preprocess\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    628\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m shape = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minternal_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    632\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m internal_name \u001b[38;5;129;01min\u001b[39;00m CHECKED_VOLUME | CHECKED_MASK3D:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/paralamg/845E4AB55E4AA032/Code/uforest/.venv/lib/python3.13/site-packages/albumentations/core/composition.py:908\u001b[39m, in \u001b[36mCompose._get_data_shape\u001b[39m\u001b[34m(self, data_name, internal_name, data)\u001b[39m\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# Handle multi-item data (masks, images, volumes)\u001b[39;00m\n\u001b[32m    907\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m internal_name \u001b[38;5;129;01min\u001b[39;00m CHECKED_MULTI:\n\u001b[32m--> \u001b[39m\u001b[32m908\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_multi_data_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minternal_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/paralamg/845E4AB55E4AA032/Code/uforest/.venv/lib/python3.13/site-packages/albumentations/core/composition.py:921\u001b[39m, in \u001b[36mCompose._get_multi_data_shape\u001b[39m\u001b[34m(self, data_name, internal_name, data)\u001b[39m\n\u001b[32m    919\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get shape of multi-item data (masks, images, volumes).\"\"\"\u001b[39;00m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m internal_name == \u001b[33m\"\u001b[39m\u001b[33mmasks\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m     shape = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_masks_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    922\u001b[39m     \u001b[38;5;66;03m# Skip empty masks lists when returning shape\u001b[39;00m\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/paralamg/845E4AB55E4AA032/Code/uforest/.venv/lib/python3.13/site-packages/albumentations/core/composition.py:806\u001b[39m, in \u001b[36mCompose._check_masks_data\u001b[39m\u001b[34m(data_name, data)\u001b[39m\n\u001b[32m    804\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(m, np.ndarray) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m data):\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAll elements in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be numpy arrays\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    807\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(m.ndim \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m} \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m data):\n\u001b[32m    808\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAll masks in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be 2D or 3D numpy arrays\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: All elements in masks must be numpy arrays"
     ]
    }
   ],
   "source": [
    "# Пример для тренировочного изображения\n",
    "image, bboxes, masks = train_dataset[0]\n",
    "print(\"Train image shape:\", image.shape)\n",
    "\n",
    "# Пример для валидационного изображения\n",
    "image, bboxes, masks = val_dataset[0]\n",
    "print(\"Val image shape:\", image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b177a69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.42s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=1.42s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def transform(image: Image, target: dict):\n",
    "    return image, target  # Apply ToTensor only to the image\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    return images, targets\n",
    "\n",
    "# Create a dataset\n",
    "train_dataset = CocoDetection(\n",
    "    root=img_folder, \n",
    "    annFile=ann_file, \n",
    "    transforms=transform\n",
    ")\n",
    "\n",
    "val_dataset = CocoDetection(\n",
    "    root=img_folder, \n",
    "    annFile=ann_file, \n",
    "    transforms=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7c73cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Image' object has no attribute 'mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m image, target = train_dataset[\u001b[32m0\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m())\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(target[\u001b[32m0\u001b[39m])\n\u001b[32m      5\u001b[39m image, target = val_dataset[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mAttributeError\u001b[39m: 'Image' object has no attribute 'mean'"
     ]
    }
   ],
   "source": [
    "image, target = train_dataset[0]\n",
    "print(image.())\n",
    "print(target[0])\n",
    "\n",
    "image, target = val_dataset[0]\n",
    "print(image.shape)\n",
    "print(image.mean())\n",
    "print(target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5cf3923",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\"background\": 0, \"tree\": 1}\n",
    "id2label = {v: k for k, v in label2id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4731f90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-tiny-coco-instance and are newly initialized because the shapes did not match:\n",
      "- class_predictor.bias: found shape torch.Size([81]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- class_predictor.weight: found shape torch.Size([81, 256]) in the checkpoint and torch.Size([3, 256]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([81]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForUniversalSegmentation,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "model = AutoModelForUniversalSegmentation.from_pretrained(\n",
    "        'facebook/mask2former-swin-tiny-coco-instance',\n",
    "        label2id=label2id,\n",
    "        id2label=id2label,\n",
    "        ignore_mismatched_sizes=True,\n",
    "\n",
    "    )\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "        'facebook/mask2former-swin-tiny-coco-instance',\n",
    "        do_resize=True,\n",
    "        size={\"height\": 2048, \"width\": 2048},\n",
    "        use_fast=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d0b71eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_augment_and_transform = A.Compose(\n",
    "        [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.5),\n",
    "            A.HueSaturationValue(p=0.1),\n",
    "        ],\n",
    "    )\n",
    "validation_transform = A.Compose(\n",
    "    [A.NoOp()],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dd711e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
